{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8bb8a4a-8b20-4475-b0d0-612ae12df768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid response from API.\n",
      "Compilation attempts: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"yourapikey",\n",
    ")\n",
    "\n",
    "# Model name\n",
    "model_name = \"google/gemini-pro-1.5-exp\"\n",
    "# Disable tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Function to read content from a .txt file\n",
    "def read_txt(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()  # Read the entire file content\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to write content to a .txt file\n",
    "def write_txt(file_path, content):\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to file: {e}\")\n",
    "\n",
    "# Function to run make command and capture output\n",
    "def run_make():\n",
    "    try:\n",
    "        result = subprocess.run([\"make\"], capture_output=True, text=True)\n",
    "        return result.returncode, result.stdout, result.stderr\n",
    "    except Exception as e:\n",
    "        print(f\"Error running make: {e}\")\n",
    "        return -1, \"\", str(e)\n",
    "\n",
    "# Read the C++ code from the files\n",
    "cpp_code = read_txt(\"/home/sl3489/ece6775_project/ece6775-final-project/finalproject/lab1/cordicGen.cpp\")\n",
    "\n",
    "\n",
    "# Create the initial prompt with the code included\n",
    "prompt = f\"Your task is to optimize the provided code cordicGen.cpp by adding HLS pragmas or directives that minimize latency while maintaining the original functionality. You can apply use set directive pipeline and/or set directive unroll commands to appropriate position in the code.  The final design should preserve functionality while achieving a significant reduction in latency. Here is the cordicGen.cpp code:\\n\\n{cpp_code}\"# Initialize variables for tracking attempts\n",
    "\n",
    "# Initialize variables for tracking attempts\n",
    "attempts = 1\n",
    "max_attempts = 10  # Set a limit to avoid infinite loops\n",
    "\n",
    "while attempts < max_attempts:\n",
    "    # Send the API request\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API request: {e}\")\n",
    "        break\n",
    "\n",
    "    # Check if the completion is valid\n",
    "    if completion and completion.choices:\n",
    "        response_content = completion.choices[0].message.content\n",
    "\n",
    "        # Extract the code part from the response\n",
    "        import re\n",
    "        code_block = re.search(r'```(.*?)```', response_content, re.DOTALL)\n",
    "        if code_block:\n",
    "            code_content = code_block.group(1).strip()\n",
    "        else:\n",
    "            code_content = response_content.strip()\n",
    "\n",
    "        # Remove any leading \"cpp\" from the code content\n",
    "        if code_content.startswith(\"cpp\"):\n",
    "            code_content = code_content[3:].strip()\n",
    "\n",
    "        # Write the code content to a file\n",
    "        output_file_path = \"/home/sl3489/ece6775_project/ece6775-final-project/finalproject/lab1/cordic.cpp\"\n",
    "        write_txt(output_file_path, code_content)\n",
    "        print(f\"Output written to {output_file_path}\")\n",
    "\n",
    "        # Run make command\n",
    "        returncode, stdout, stderr = run_make()\n",
    "        print(stdout)\n",
    "        print(stderr)\n",
    "\n",
    "        if returncode == 0:\n",
    "            print(\"Compilation successful!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Compilation failed. Updating prompt with error messages.\")\n",
    "            prompt += f\"\\n\\nThe compilation failed with the following errors:\\n{stderr}\"\n",
    "            attempts += 1\n",
    "    else:\n",
    "        print(\"Invalid response from API.\")\n",
    "        break\n",
    "\n",
    "print(f\"Compilation attempts: {attempts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a045bb-c44c-483b-82f2-ab9759e54fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
